---
title: "ApprentissageSTAN"
author: "Eric"
date: "3/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Beta-Bernoulli model

$y\sim Bern(\theta)$ et $\theta \sim Beta(\alpha, \beta)$
```{r Beta-Binomial}
library(rstan)
bernoulli <- "
	data { 
	  real<lower=0.5> alpha;
	  real<lower=0.5> beta;
		int<lower=0> N; 
		int<lower=0,upper=1> y[N];
	} 
	parameters {
		real<lower=0,upper=1> theta;
	} 
	model {
		theta ~ beta(alpha,beta);
		for (n in 1:N) 
		y[n] ~ bernoulli(theta);
	}"

N <- 100
y <- rbinom(N, 1, .4)
data <- list(y=y, N=N,alpha=1, beta=1)
m <- stan_model(model_code = bernoulli)
samples <- sampling(m, data=data, iter=1000, chains=1)
theta <- mean(extract(samples)$theta)
```

## Poisson-Gamma

$y\sim Poisson(\lambda)$ et $\lambda \sim Gamma(\alpha, \beta)$

```{r Poisson-Gamma, echo=FALSE}
#require(rstan)
poisson <- "
	data {
	real<lower=0> alpha;
	real<lower=0> beta;
	int<lower=0> N; 
	int<lower=0> y[N];
	}
	parameters { 
	real <lower=0>lambda;
	}
	model { 
	lambda ~ gamma(alpha,beta);
		for (n in 1:N) 
		y[n] ~ poisson(lambda);
	}"


N <- 100
lambda <- 10
y <- rpois(N,lambda)
data <- list(y=y, N=N, alpha=1, beta=1)
m <- stan_model(model_code = poisson)
samples <- sampling(m, data=data, iter=1000, chains=1)
lambda <- mean(extract(samples)$lambda)

```

## ConjuguÃ©e normal

$y \sim \mathcal{N}(\mu,\sigma^2)$ $\mu \sim \mathcal{N}(\mu_0,\sigma_0^2)$

```{r normal}

normal <- "
	data {		
  real mu_0;
	real<lower=0> sigma_0;
	real<lower=0> sigma;
	int<lower=0> N; 
	vector[N] y;
	}
	parameters {
	real mu;
	}
	model {
	  mu ~ normal(mu_0,sigma_0);
		y ~ normal(mu,sigma);
	}"

mu <- 10
n <- 100
y <- rnorm(N, 10, 3)
data <- list(y=y, mu_0= 8, sigma_0=2, N=n, sigma=3)
m <- stan_model(model_code = normal)
samples <- sampling(m, data=data, iter=1000, chains=1)
mu <- mean(extract(samples)$mu)
```


## Normal
$$y \sim \mathcal{N}(\mu,\sigma^2)\\
\mu \sim \mathcal{N}(\mu_0,\sigma_0^2)\\
\sigma \sim Cauchy(\alpha, \beta)$$

```{r}
normal <- "
	data {		
		int<lower=0> n; 
		vector[n] y;
	}
	parameters {
		real mu;
		real<lower=0> sigma;
	}
	model {
		mu ~ normal(0,1);
        sigma ~ cauchy(0,10);
		y ~ normal(mu, sigma);
	}"

mu <- 10
sigma <- 5
n <- 1000
y <- rnorm(n, mu, sigma)
data <- list(y=y, n=n)
m <- stan_model(model_code = normal)
samples <- sampling(m, data=data, iter=2000, chains=3)
mu <- mean(extract(samples)$mu)
sigma <- mean(extract(samples)$sigma)
ggmcmc::ggs_density(ggmcmc::ggs(samples))
```


## Bayesian linear regression with Automatic Relevance Determination#

```{r}
#require(rstan)
bayesian_linear_ard <- "
	data {
		int<lower=0> N;
		int<lower=0> D;
		matrix[N,D] X;
		vector[N] y;
	}
	parameters {
		vector[D] w;
		vector<lower=0>[D] alpha;
		real<lower=0> tau;
	}
	transformed parameters {
		vector<lower=0>[D] t_alpha;
		real<lower=0> t_tau;
		for (d in 1:D) t_alpha[d] = 1/sqrt(alpha[d]);
		t_tau =  1/sqrt(tau);
	}
	model {
		tau ~ gamma(1, 1);
		alpha ~ gamma(1e-3,1e-3);
		w ~ normal(0,  t_alpha);
		y ~ normal(X*w, t_tau);
	}"

tau <- 1
N <- 1000
D <- 10
alpha <- rep(1,D)
alpha[1:5] <- 1e6
w <- sapply(1/sqrt(alpha), function(a) rnorm(1,sd=a))
X <- matrix(rnorm(N*D), N, D)
y <- c(X %*% w + rnorm(N))
data <- list(N=N, D=D, X=X, y=y)

m <- stan_model(model_code = bayesian_linear_ard)
samples <- sampling(m, data=data, iter=2000, chains=1)
w <- colMeans(extract(samples)$w)
alpha <- colMeans(extract(samples)$alpha)
tau <- mean(extract(samples)$tau)
```

## Logit regression with ARD  
```{r}
#require(rstan)
require(boot)
set.seed(100)
bayesian_logistic_ard <- "
	data {
		int<lower=0> N;
		int<lower=0> D;
		matrix[N,D] X;
		int<lower=0,upper=1> y[N];
	}
	parameters {
		vector[D] w;
		vector<lower=0>[D] alpha;
	}
	transformed parameters {
		vector<lower=0>[D] t_alpha;
		for (d in 1:D) t_alpha[d] = 1/sqrt(alpha[d]);
	}
	model {
		alpha ~ gamma(1e-3,1e-3);
		w ~ normal(0, t_alpha);
		y ~ bernoulli_logit(X*w);
	}"


tau <- 1
N <- 1000
D <- 10
alpha <- rep(1,D)
alpha[1:5] <- 1e6
w <- sapply(1/sqrt(alpha), function(a) rnorm(1,sd=a))
X <- matrix(rnorm(N*D), N, D)
y <- rbinom(N,1, inv.logit(c(X %*% w + rnorm(N))))
data <- list(N=N, D=D, X=X, y=y)

m <- stan_model(model_code = bayesian_logistic_ard)
samples <- sampling(m, data=data, iter=2000, chains=3)
samples <- vb(m, data = data, algorithm = "meanfield")
w <- colMeans(extract(samples)$w)
alpha <- colMeans(extract(samples)$alpha)
```



## Bayesian PCA


```{r}
#require(rstan)
require(gplots)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

set.seed(100)
pca <- "
	data {
		int<lower=0> N; // Number of samples
		int<lower=0> D; // The original dimension
		int<lower=0> K; // The latent dimension
		matrix[N, D] X; // The data matrix
	}

	parameters {
		matrix[N, K] Z; // The latent matrix
		matrix[D, K] W; // The weight matrix
		real<lower=0> tau; // Noise term 
		vector<lower=0>[K] alpha; // ARD prior
	}

	transformed parameters{
		vector<lower=0>[K] t_alpha;
		real<lower=0> t_tau;
                t_alpha = inv(sqrt(alpha));
                t_tau = inv(sqrt(tau));
	}
	model {
		tau ~ gamma(1,1);			
		to_vector(Z) ~ normal(0,1);
		alpha ~ gamma(1e-3,1e-3);				
		for(k in 1:K) W[,k] ~ normal(0, t_alpha[k]);
		to_vector(X) ~ normal(to_vector(Z*W'), t_tau);

	} "


N <- 400#200
D <- 200#20
K <- 5
Z <- matrix(rnorm(N*K,0,1),N,K)    # Latent components
tau <- 3
alpha <- rep(1,K)    # Component precisions for the two data sets

W <- matrix(0,D,K)   # The weights
for(k in 1:K)  W[,k] <- rnorm(D,0,1/sqrt(alpha[k]))
X <- Z %*% t(W) + matrix(rnorm(N*D,0,1/sqrt(tau)),N,D)   
data <- list(N = N, D = D, K = 10, X = X)

m <- stan_model(model_code = pca)
stan.fit.vb <- vb(m, data = data, algorithm = "meanfield")
W.vb <- apply(extract(stan.fit.vb,"W")[[1]], c(2,3), mean)
alpha.vb <- apply(extract(stan.fit.vb,"alpha")[[1]], c(2), mean)

heatmap.2(W.vb, col = bluered(70), dendrogram='none',trace='none', Rowv = FALSE, Colv = FALSE, key=FALSE)

#stan.fit.sampling <- sampling(m, data = data, chains=1, iter=1000); 
#W.sampling <- t(apply(extract(stan.fit.sampling,"W")[[1]], c(2,3), mean))
#alpha.sampling <- apply(extract(stan.fit.sampling,"alpha")[[1]], c(2), mean)
#heatmap.2(W.sampling, col = bluered(70), dendrogram='none',trace='none', Rowv = FALSE, Colv = FALSE, key=FALSE)

load("UML.RData")
X <- GeneExpression.HL60
N <- dim(X)[1]
D <- dim(X)[2]
K <- 5
data <- list(N = N, D = D, K = K, X = X)

m <- stan_model(model_code = pca)
stan.fit.vb.real <- vb(m, data = data, algorithm = "meanfield", iter = 5000)
alpha.vb.real <- apply(extract(stan.fit.vb.real,"alpha")[[1]], c(2), mean)
Z.vb <- apply(extract(stan.fit.vb.real,"Z")[[1]], c(2,3), mean)
W.vb <- apply(extract(stan.fit.vb.real,"W")[[1]], c(2,3), mean)

```

